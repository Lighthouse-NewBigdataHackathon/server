{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "evij7LOqc2ry",
    "outputId": "28d5144a-5e9c-4596-8464-901d8300fbbf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.8.1+cpu in /usr/local/lib/python3.7/dist-packages (1.8.1+cpu)\n",
      "Requirement already satisfied: torchvision==0.9.1+cpu in /usr/local/lib/python3.7/dist-packages (0.9.1+cpu)\n",
      "Requirement already satisfied: torchaudio==0.8.1 in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cpu) (1.21.6)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1+cpu) (4.1.1)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from torchvision==0.9.1+cpu) (7.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch==1.8.1+cpu torchvision==0.9.1+cpu torchaudio==0.8.1 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PTRdSO9HrLtz",
    "outputId": "a0999dfd-8448-4130-f7ba-bd651f63f3ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hVuplEqWg2Om"
   },
   "source": [
    "# Load pyrouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8JOBFcBn_OLz",
    "outputId": "62766cc2-16b5-459f-9c05-89bf7a0d1896"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pyrouge in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
      "  Using cached https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: pyrouge in /usr/local/lib/python3.7/dist-packages (0.1.3)\n",
      "Name: pyrouge\n",
      "Version: 0.1.3\n",
      "Summary: A Python wrapper for the ROUGE summarization evaluation package.\n",
      "Home-page: https://github.com/noutenki/pyrouge\n",
      "Author: Benjamin Heinzerling, Anders Johannsen\n",
      "Author-email: benjamin.heinzerling@h-its.org\n",
      "License: LICENSE.txt\n",
      "Location: /usr/local/lib/python3.7/dist-packages\n",
      "Requires: \n",
      "Required-by: \n",
      "Cloning into 'pyrouge'...\n",
      "remote: Enumerating objects: 393, done.\u001b[K\n",
      "remote: Total 393 (delta 0), reused 0 (delta 0), pack-reused 393\u001b[K\n",
      "Receiving objects: 100% (393/393), 298.74 KiB | 15.72 MiB/s, done.\n",
      "Resolving deltas: 100% (109/109), done.\n",
      "2022-09-16 02:27:53,337 [MainThread  ] [INFO ]  Set ROUGE home directory to pyrouge/tools/ROUGE-1.5.5.\n"
     ]
    }
   ],
   "source": [
    "!pip install pyrouge --upgrade\n",
    "!pip install https://github.com/bheinzerling/pyrouge/archive/master.zip\n",
    "!pip install pyrouge\n",
    "!pip show pyrouge\n",
    "!git clone https://github.com/andersjo/pyrouge.git\n",
    "from pyrouge import Rouge155\n",
    "!pyrouge_set_rouge_path 'pyrouge/tools/ROUGE-1.5.5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3BxQRUc5KeGG",
    "outputId": "ae27e8a2-33cc-405b-9f0f-a73eb3dea553"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.22.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: tensorboardX in /usr/local/lib/python3.7/dist-packages (2.5.1)\n",
      "Requirement already satisfied: protobuf<=3.20.1,>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.21.6)\n",
      "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf<=3.20.1,>=3.8.0->tensorboardX) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: easydict in /usr/local/lib/python3.7/dist-packages (1.9)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!pip install tensorboardX\n",
    "!pip install easydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "amSkThJD-Bxx",
    "outputId": "1b6c81f7-7987-482c-f7ea-7eef3432a001"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n",
      "fatal: destination path 'KorBertSum' already exists and is not an empty directory.\n",
      "/content/KorBertSum\n"
     ]
    }
   ],
   "source": [
    "%cd /content\n",
    "!git clone https://github.com/HaloKim/KorBertSum.git\n",
    "%cd /content/KorBertSum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m-yEOMhAAPWs"
   },
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "WkUneUqUTtZ9"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.chdir('/content/KorBertSum/src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "2ggKByDgUfp8"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from models import data_loader, model_builder\n",
    "from models.model_builder import Summarizer\n",
    "from others.logging import logger, init_logger\n",
    "from models.data_loader import load_dataset\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from tensorboardX import SummaryWriter\n",
    "from models.reporter import ReportMgr\n",
    "from models.stats import Statistics\n",
    "import easydict\n",
    "import requests,json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9qjyO4fND_h"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "PeRgtmVDLgRy"
   },
   "outputs": [],
   "source": [
    "def _tally_parameters(model):\n",
    "    n_params = sum([p.nelement() for p in model.parameters()])\n",
    "    return n_params\n",
    "\n",
    "def build_trainer(args, device_id, model,\n",
    "                  optim):\n",
    "    \"\"\"\n",
    "    Simplify `Trainer` creation based on user `opt`s*\n",
    "    Args:\n",
    "        opt (:obj:`Namespace`): user options (usually from argument parsing)\n",
    "        model (:obj:`onmt.models.NMTModel`): the model to train\n",
    "        fields (dict): dict of fields\n",
    "        optim (:obj:`onmt.utils.Optimizer`): optimizer used during training\n",
    "        data_type (str): string describing the type of data\n",
    "            e.g. \"text\", \"img\", \"audio\"\n",
    "        model_saver(:obj:`onmt.models.ModelSaverBase`): the utility object\n",
    "            used to save the model\n",
    "    \"\"\"\n",
    "    device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "\n",
    "\n",
    "    grad_accum_count = args.accum_count\n",
    "    n_gpu = args.world_size\n",
    "\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = int(args.gpu_ranks[device_id])\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "\n",
    "    print('gpu_rank %d' % gpu_rank)\n",
    "\n",
    "    tensorboard_log_dir = args.model_path\n",
    "\n",
    "    writer = SummaryWriter(tensorboard_log_dir, comment=\"Unmt\")\n",
    "\n",
    "    report_manager = ReportMgr(args.report_every, start_time=-1, tensorboard_writer=writer)\n",
    "\n",
    "    trainer = Trainer(args, model, optim, grad_accum_count, n_gpu, gpu_rank, report_manager)\n",
    "\n",
    "    # print(tr)\n",
    "    if (model):\n",
    "        n_params = _tally_parameters(model)\n",
    "        logger.info('* number of parameters: %d' % n_params)\n",
    "\n",
    "    return trainer\n",
    "\n",
    "\n",
    "class Trainer(object):\n",
    "    \"\"\"\n",
    "    Class that controls the training process.\n",
    "\n",
    "    Args:\n",
    "            model(:py:class:`onmt.models.model.NMTModel`): translation model\n",
    "                to train\n",
    "            train_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            valid_loss(:obj:`onmt.utils.loss.LossComputeBase`):\n",
    "               training loss computation\n",
    "            optim(:obj:`onmt.utils.optimizers.Optimizer`):\n",
    "               the optimizer responsible for update\n",
    "            trunc_size(int): length of truncated back propagation through time\n",
    "            shard_size(int): compute loss in shards of this size for efficiency\n",
    "            data_type(string): type of the source input: [text|img|audio]\n",
    "            norm_method(string): normalization methods: [sents|tokens]\n",
    "            grad_accum_count(int): accumulate gradients this many times.\n",
    "            report_manager(:obj:`onmt.utils.ReportMgrBase`):\n",
    "                the object that creates reports, or None\n",
    "            model_saver(:obj:`onmt.models.ModelSaverBase`): the saver is\n",
    "                used to save a checkpoint.\n",
    "                Thus nothing will be saved if this parameter is None\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,  args, model,  optim,\n",
    "                  grad_accum_count=1, n_gpu=1, gpu_rank=1,\n",
    "                  report_manager=None):\n",
    "        # Basic attributes.\n",
    "        self.args = args\n",
    "        self.save_checkpoint_steps = args.save_checkpoint_steps\n",
    "        self.model = model\n",
    "        self.optim = optim\n",
    "        self.grad_accum_count = grad_accum_count\n",
    "        self.n_gpu = n_gpu\n",
    "        self.gpu_rank = gpu_rank\n",
    "        self.report_manager = report_manager\n",
    "\n",
    "        self.loss = torch.nn.BCELoss(reduction='none')\n",
    "        assert grad_accum_count > 0\n",
    "        # Set model in training mode.\n",
    "        if (model):\n",
    "            self.model.train()\n",
    "\n",
    "    def summ(self, test_iter, step, cal_lead=False, cal_oracle=False):\n",
    "      \"\"\" Validate model.\n",
    "          valid_iter: validate data iterator\n",
    "      Returns:\n",
    "          :obj:`nmt.Statistics`: validation loss statistics\n",
    "      \"\"\"\n",
    "      # Set model in validating mode.\n",
    "      def _get_ngrams(n, text):\n",
    "          ngram_set = set()\n",
    "          text_length = len(text)\n",
    "          max_index_ngram_start = text_length - n\n",
    "          for i in range(max_index_ngram_start + 1):\n",
    "              ngram_set.add(tuple(text[i:i + n]))\n",
    "          return ngram_set\n",
    "\n",
    "      def _block_tri(c, p):\n",
    "          tri_c = _get_ngrams(3, c.split())\n",
    "          for s in p:\n",
    "              tri_s = _get_ngrams(3, s.split())\n",
    "              if len(tri_c.intersection(tri_s))>0:\n",
    "                  return True\n",
    "          return False\n",
    "\n",
    "      if (not cal_lead and not cal_oracle):\n",
    "          self.model.eval()\n",
    "      stats = Statistics()\n",
    "\n",
    "      with torch.no_grad():\n",
    "          for batch in test_iter:\n",
    "              src = batch.src\n",
    "              labels = batch.labels\n",
    "              segs = batch.segs\n",
    "              clss = batch.clss\n",
    "              mask = batch.mask\n",
    "              mask_cls = batch.mask_cls\n",
    "\n",
    "              if (cal_lead):\n",
    "                  selected_ids = [list(range(batch.clss.size(1)))] * batch.batch_size\n",
    "              elif (cal_oracle):\n",
    "                  selected_ids = [[j for j in range(batch.clss.size(1)) if labels[i][j] == 1] for i in\n",
    "                                  range(batch.batch_size)]\n",
    "              else:\n",
    "                  sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "                  sent_scores = sent_scores + mask.float()\n",
    "                  sent_scores = sent_scores.cpu().data.numpy()\n",
    "                  selected_ids = np.argsort(-sent_scores, 1)\n",
    "      return selected_ids\n",
    "\n",
    "\n",
    "\n",
    "    def _gradient_accumulation(self, true_batchs, normalization, total_stats,\n",
    "                               report_stats):\n",
    "        if self.grad_accum_count > 1:\n",
    "            self.model.zero_grad()\n",
    "\n",
    "        for batch in true_batchs:\n",
    "            if self.grad_accum_count == 1:\n",
    "                self.model.zero_grad()\n",
    "\n",
    "            src = batch.src\n",
    "            labels = batch.labels\n",
    "            segs = batch.segs\n",
    "            clss = batch.clss\n",
    "            mask = batch.mask\n",
    "            mask_cls = batch.mask_cls\n",
    "\n",
    "            sent_scores, mask = self.model(src, segs, clss, mask, mask_cls)\n",
    "\n",
    "            loss = self.loss(sent_scores, labels.float())\n",
    "            loss = (loss*mask.float()).sum()\n",
    "            (loss/loss.numel()).backward()\n",
    "            # loss.div(float(normalization)).backward()\n",
    "\n",
    "            batch_stats = Statistics(float(loss.cpu().data.numpy()), normalization)\n",
    "\n",
    "\n",
    "            total_stats.update(batch_stats)\n",
    "            report_stats.update(batch_stats)\n",
    "\n",
    "            # 4. Update the parameters and statistics.\n",
    "            if self.grad_accum_count == 1:\n",
    "                # Multi GPU gradient gather\n",
    "                if self.n_gpu > 1:\n",
    "                    grads = [p.grad.data for p in self.model.parameters()\n",
    "                             if p.requires_grad\n",
    "                             and p.grad is not None]\n",
    "                    distributed.all_reduce_and_rescale_tensors(\n",
    "                        grads, float(1))\n",
    "                self.optim.step()\n",
    "\n",
    "        # in case of multi step gradient accumulation,\n",
    "        # update only after accum batches\n",
    "        if self.grad_accum_count > 1:\n",
    "            if self.n_gpu > 1:\n",
    "                grads = [p.grad.data for p in self.model.parameters()\n",
    "                         if p.requires_grad\n",
    "                         and p.grad is not None]\n",
    "                distributed.all_reduce_and_rescale_tensors(\n",
    "                    grads, float(1))\n",
    "            self.optim.step()\n",
    "\n",
    "    def _save(self, step):\n",
    "        real_model = self.model\n",
    "        # real_generator = (self.generator.module\n",
    "        #                   if isinstance(self.generator, torch.nn.DataParallel)\n",
    "        #                   else self.generator)\n",
    "\n",
    "        model_state_dict = real_model.state_dict()\n",
    "        # generator_state_dict = real_generator.state_dict()\n",
    "        checkpoint = {\n",
    "            'model': model_state_dict,\n",
    "            # 'generator': generator_state_dict,\n",
    "            'opt': self.args,\n",
    "            'optim': self.optim,\n",
    "        }\n",
    "        checkpoint_path = os.path.join(self.args.model_path, 'model_step_%d.pt' % step)\n",
    "        logger.info(\"Saving checkpoint %s\" % checkpoint_path)\n",
    "        # checkpoint_path = '%s_step_%d.pt' % (FLAGS.model_path, step)\n",
    "        if (not os.path.exists(checkpoint_path)):\n",
    "            torch.save(checkpoint, checkpoint_path)\n",
    "            return checkpoint, checkpoint_path\n",
    "\n",
    "    def _start_report_manager(self, start_time=None):\n",
    "        \"\"\"\n",
    "        Simple function to start report manager (if any)\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            if start_time is None:\n",
    "                self.report_manager.start()\n",
    "            else:\n",
    "                self.report_manager.start_time = start_time\n",
    "\n",
    "    def _maybe_gather_stats(self, stat):\n",
    "        \"\"\"\n",
    "        Gather statistics in multi-processes cases\n",
    "\n",
    "        Args:\n",
    "            stat(:obj:onmt.utils.Statistics): a Statistics object to gather\n",
    "                or None (it returns None in this case)\n",
    "\n",
    "        Returns:\n",
    "            stat: the updated (or unchanged) stat object\n",
    "        \"\"\"\n",
    "        if stat is not None and self.n_gpu > 1:\n",
    "            return Statistics.all_gather_stats(stat)\n",
    "        return stat\n",
    "\n",
    "    def _maybe_report_training(self, step, num_steps, learning_rate,\n",
    "                               report_stats):\n",
    "        \"\"\"\n",
    "        Simple function to report training stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_training` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_training(\n",
    "                step, num_steps, learning_rate, report_stats,\n",
    "                multigpu=self.n_gpu > 1)\n",
    "\n",
    "    def _report_step(self, learning_rate, step, train_stats=None,\n",
    "                     valid_stats=None):\n",
    "        \"\"\"\n",
    "        Simple function to report stats (if report_manager is set)\n",
    "        see `onmt.utils.ReportManagerBase.report_step` for doc\n",
    "        \"\"\"\n",
    "        if self.report_manager is not None:\n",
    "            return self.report_manager.report_step(\n",
    "                learning_rate, step, train_stats=train_stats,\n",
    "                valid_stats=valid_stats)\n",
    "\n",
    "    def _maybe_save(self, step):\n",
    "        \"\"\"\n",
    "        Save the model if a model saver is set\n",
    "        \"\"\"\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.maybe_save(step)\n",
    "\n",
    "class BertData():\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "        self.sep_vid = self.tokenizer.vocab['[SEP]']\n",
    "        self.cls_vid = self.tokenizer.vocab['[CLS]']\n",
    "        self.pad_vid = self.tokenizer.vocab['[PAD]']\n",
    "\n",
    "    def preprocess(self, src):\n",
    "\n",
    "        if (len(src) == 0):\n",
    "            return None\n",
    "\n",
    "        original_src_txt = [' '.join(s) for s in src]\n",
    "        idxs = [i for i, s in enumerate(src) if (len(s) > 1)]\n",
    "\n",
    "        src = [src[i][:2000] for i in idxs]\n",
    "        src = src[:1000]\n",
    "\n",
    "        if (len(src) < 3):\n",
    "            return None\n",
    "\n",
    "        src_txt = [' '.join(sent) for sent in src]\n",
    "        text = ' [SEP] [CLS] '.join(src_txt)\n",
    "        src_subtokens = self.tokenizer.tokenize(text)\n",
    "        src_subtokens = src_subtokens[:510]\n",
    "        src_subtokens = ['[CLS]'] + src_subtokens + ['[SEP]']\n",
    "\n",
    "        src_subtoken_idxs = self.tokenizer.convert_tokens_to_ids(src_subtokens)\n",
    "        _segs = [-1] + [i for i, t in enumerate(src_subtoken_idxs) if t == self.sep_vid]\n",
    "        segs = [_segs[i] - _segs[i - 1] for i in range(1, len(_segs))]\n",
    "        segments_ids = []\n",
    "        for i, s in enumerate(segs):\n",
    "            if (i % 2 == 0):\n",
    "                segments_ids += s * [0]\n",
    "            else:\n",
    "                segments_ids += s * [1]\n",
    "        cls_ids = [i for i, t in enumerate(src_subtoken_idxs) if t == self.cls_vid]\n",
    "        labels = None\n",
    "        src_txt = [original_src_txt[i] for i in idxs]\n",
    "        tgt_txt = None\n",
    "        return src_subtoken_idxs, labels, segments_ids, cls_ids, src_txt, tgt_txt\n",
    "\n",
    "def _lazy_dataset_loader(pt_file):\n",
    "  yield  pt_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pC_QmVS6NGvy"
   },
   "source": [
    "# Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "6tzZYPbLbRl7"
   },
   "outputs": [],
   "source": [
    "args = easydict.EasyDict({\n",
    "    \"encoder\":'classifier',\n",
    "    \"mode\":'test',\n",
    "    \"bert_data_path\":'/content/drive/MyDrive/Colab Notebooks/BertSum-master/bert_data/korean',\n",
    "    \"model_path\":'./models/bert_classifier',\n",
    "    \"result_path\":'./results',\n",
    "    \"temp_dir\":'./temp',\n",
    "    \"batch_size\":1000,\n",
    "    \"use_interval\":True,\n",
    "    \"hidden_size\":128,\n",
    "    \"ff_size\":512,\n",
    "    \"heads\":4,\n",
    "    \"inter_layers\":2,\n",
    "    \"rnn_size\":512,\n",
    "    \"param_init\":0,\n",
    "    \"param_init_glorot\":True,\n",
    "    \"dropout\":0.1,\n",
    "    \"optim\":'adam',\n",
    "    \"lr\":2e-3,\n",
    "    \"report_every\":1,\n",
    "    \"save_checkpoint_steps\":5,\n",
    "    \"block_trigram\":True,\n",
    "    \"recall_eval\":False,\n",
    "    \n",
    "    \"accum_count\":1,\n",
    "    \"world_size\":1,\n",
    "    \"visible_gpus\":'-1',\n",
    "    \"gpu_ranks\":'0',\n",
    "    \"log_file\":'/content/drive/MyDrive/Colab Notebooks/KorBertSum-master/logs/log.txt',\n",
    "    \"test_from\":'/content/drive/MyDrive/model_step_100000.pt'\n",
    "})\n",
    "model_flags = ['hidden_size', 'ff_size', 'heads', 'inter_layers','encoder','ff_actv', 'use_interval','rnn_size']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aJ1pvUQBNITZ"
   },
   "source": [
    "# Test code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "WYV7oXExF2je"
   },
   "outputs": [],
   "source": [
    "def test(args, input_list, device_id, pt, step):\n",
    "  init_logger(args.log_file)\n",
    "  device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "  device_id = 0 if device == \"cuda\" else -1\n",
    "\n",
    "  cp = args.test_from\n",
    "  try:\n",
    "    step = int(cp.split('.')[-2].split('_')[-1])\n",
    "  except:\n",
    "    step = 0\n",
    "\n",
    "  device = \"cpu\" if args.visible_gpus == '-1' else \"cuda\"\n",
    "  if (pt != ''):\n",
    "      test_from = pt\n",
    "  else:\n",
    "      test_from = args.test_from\n",
    "  logger.info('Loading checkpoint from %s' % test_from)\n",
    "  checkpoint = torch.load(test_from, map_location=lambda storage, loc: storage)\n",
    "  opt = vars(checkpoint['opt'])\n",
    "  for k in opt.keys():\n",
    "      if (k in model_flags):\n",
    "        setattr(args, k, opt[k])\n",
    "\n",
    "  config = BertConfig.from_pretrained('bert-base-multilingual-cased')\n",
    "  model = Summarizer(args, device, load_pretrained_bert=False, bert_config = config)\n",
    "  model.load_cp(checkpoint)\n",
    "  model.eval()\n",
    "\n",
    "  test_iter = data_loader.Dataloader(args, _lazy_dataset_loader(input_list),\n",
    "                                args.batch_size, device,\n",
    "                                shuffle=False, is_test=True)\n",
    "  trainer = build_trainer(args, device_id, model, None)\n",
    "  result = trainer.summ(test_iter,step)\n",
    "  return result, input_list\n",
    "\n",
    "args.gpu_ranks = [int(i) for i in args.gpu_ranks.split(',')]\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = args.visible_gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "AywahUrUkPzB"
   },
   "outputs": [],
   "source": [
    "def txt2input(text):\n",
    "  data = list(filter(None, text.split('.')))\n",
    "  bertdata = BertData()   \n",
    "  txt_data = bertdata.preprocess(data)\n",
    "  data_dict = {\"src\":txt_data[0],\n",
    "               \"labels\":[0,1,2],\n",
    "               \"segs\":txt_data[2],\n",
    "               \"clss\":txt_data[3],\n",
    "               \"src_txt\":txt_data[4],\n",
    "               \"tgt_txt\":None}\n",
    "  input_data = []\n",
    "  input_data.append(data_dict)\n",
    "  return input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uXZk2tZvePTK",
    "outputId": "cb65de01-55e4-4924-9875-4df78e53d7d8"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e5482407c6e5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[0murl_issue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"http://tools.kinds.or.kr:8888/issue_ranking\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mres_issue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl_issue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpayload_issue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[0mhong_issue\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mres_issue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m                      \u001b[1;31m#json파일로 받은 data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mhongimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhong_issue\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'return_object'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'topics'\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m#return\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "#API에서 가져온 text를 집어넣는 CODE\n",
    "\n",
    "#issue_ranking code : news code 추출용 \n",
    "\n",
    "payload_issue = {\"access_key\":\"\",   # api key 넣는곳 \n",
    "       \"argument\": {\n",
    "           \"date\": \"2022-09-13\",\n",
    "            \"provider\": [\"국민일보\"\n",
    "            ]\n",
    "        }\n",
    "}\n",
    "url_issue = \"http://tools.kinds.or.kr:8888/issue_ranking\"\n",
    "res_issue = requests.post(url_issue,data=json.dumps(payload_issue))\n",
    "hong_issue= res_issue.json()                      #json파일로 받은 data\n",
    "hongimg = hong_issue['return_object']['topics']   \n",
    "b = hongimg[0]['news_cluster']                    #b는 return_object의 topics의 list내의 news_cluster list\n",
    "issue_news = b[4]                                 #issue_news라는 변수내에 b[4]를 넣고 test\n",
    "                                                  #어떤 방식으로 쓰일지 몰라 우선은 가시적으로 확인하기 위해 정수를 넣어 코드작성함\n",
    "\n",
    "\n",
    "#news_code로 기사 가져오기\n",
    "payload = {\n",
    "    \"access_key\": \"\",          #api key 넣는곳\n",
    "    \"argument\": {\n",
    "        \"news_ids\": [\n",
    "            \"{0}\".format(issue_news)     #issue_news 변수로 news_id가져오기\n",
    "        ],\n",
    "        \"fields\": [\n",
    "            \"content\",\n",
    "            \"byline\",\n",
    "            \"category\",\n",
    "            \"category_incident\",\n",
    "            \"images\",\n",
    "            \"provider_subject\",\n",
    "            \"provider_news_id\",\n",
    "            \"publisher_code\"\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "url = \"http://tools.kinds.or.kr:8888/search/news\"\n",
    "res = requests.post(url, data=json.dumps(payload))\n",
    "hong = res.json()\n",
    "hongtext = (hong['return_object']['documents'])\n",
    "jitext = (hongtext[0]['content'])                 #json 내의 return_object안의 documents list 내부의\n",
    "jitext = jitext.split(\".\")                        #content(기사)를 가져옴\n",
    "realtext = \"\"\n",
    "for i in range(0,len(jitext)):\n",
    "    realtext = realtext + jitext[i]+\".\"          #마침표를 기준으로 기사를 split하여 다시 마침표를 붙여 넣어줌\n",
    "print(realtext)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7IlXBocEMzS_",
    "outputId": "987fc851-7e9d-41b0-ea06-10a2a466e1f4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2022-09-16 03:00:06,947 INFO] Loading checkpoint from /content/drive/MyDrive/model_step_100000.pt\n",
      "[2022-09-16 03:00:14,364 INFO] * number of parameters: 177854209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_rank 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0,  9,  4, 11, 12,  3, 10,  5,  1,  8,  2,  6,  7]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data = txt2input(realtext)\n",
    "sum_list = test(args, input_data, -1, '', None)\n",
    "sum_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3uOvhjs8M1-s"
   },
   "source": [
    "# Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nIhOEG0HlMpE",
    "outputId": "a3d808c0-3a99-49d3-f145-ff8dcb1f6b41"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['국민의힘 ‘정진석 비상대책위원회’ 위원 명단에 호남 출신인 주기환 전 대검찰청 검찰수사관이 포함된 것으로 13일 확인됐다',\n",
       " ' \\n \\n주 전 수사관은 윤 대통령이 2003년 광주지검에 근무할 당시 검찰수사관으로 인연을 맺었다',\n",
       " ' \\n \\n주 전 수사관은 법원이 주호영 비대위원장에 대한 직무정지를 결정한 이후 지난 5일 나머지 비대위원과 함께 사퇴했다']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[list(filter(None, realtext.split('.')))[i] for i in sum_list[0][0][:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "CSVtNDS2Jiaa"
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "QLnyCkviYuIv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
